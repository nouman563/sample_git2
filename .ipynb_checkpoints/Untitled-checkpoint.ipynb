{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66c7bee-7c0d-4ce0-bae0-a9e696aa4703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading DeepWeeds models to ./models/models.zip\n",
      "Finished downloading models.\n",
      "Error in downloading resources: Error downloading or unzipping models: ./models/models.zip is not a valid ZIP file. Check the download URL or file integrity.\n",
      "Error: You must supply a .hdf5 model file for inference.\n",
      "Error: Model file resnet does not exist.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at resnet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 389\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Model file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    388\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 389\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m inference(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\saving\\legacy\\save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    237\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at resnet"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import csv\n",
    "\n",
    "# Global paths\n",
    "OUTPUT_DIRECTORY = \"./outputs/\"\n",
    "LABEL_DIRECTORY = \"./labels/\"\n",
    "MODEL_DIRECTORY = \"./models/\"\n",
    "MODEL_GD_ID = \"1MRbN5hXOTYnw7-71K-2vjY01uJ9GkQM5\"\n",
    "MODEL_ZIP_FILE = \"./models/models.zip\"\n",
    "IMG_DIRECTORY = \"./images/\"\n",
    "IMG_GD_ID = \"1xnK3B6K6KekDI55vwJ0vnc2IGoDga9cj\"\n",
    "IMG_ZIP_FILE = \"./images/images.zip\"\n",
    "\n",
    "# Global variables\n",
    "RAW_IMG_SIZE = (256, 256)\n",
    "IMG_SIZE = (224, 224)\n",
    "INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "MAX_EPOCH = 200\n",
    "BATCH_SIZE = 32\n",
    "FOLDS = 5\n",
    "STOPPING_PATIENCE = 32\n",
    "LR_PATIENCE = 16\n",
    "INITIAL_LR = 0.0001\n",
    "CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "CLASS_NAMES = ['Chinee Apple', 'Lantana', 'Parkinsonia', 'Parthenium', \n",
    "               'Prickly Acacia', 'Rubber Vine', 'Siam Weed', 'Snake Weed', 'Negatives']\n",
    "\n",
    "def download_google_drive_file(file_id, destination):\n",
    "    \"\"\"Download a file from Google Drive.\"\"\"\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': file_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    \"\"\"Extract confirmation token from response cookies.\"\"\"\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    \"\"\"Save response content to a file.\"\"\"\n",
    "    CHUNK_SIZE = 32768\n",
    "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "    try:\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to save file to {destination}: {e}\")\n",
    "\n",
    "def validate_zip_file(file_path):\n",
    "    \"\"\"Check if a file is a valid ZIP file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} does not exist.\")\n",
    "    if not zipfile.is_zipfile(file_path):\n",
    "        raise ValueError(f\"{file_path} is not a valid ZIP file. Check the download URL or file integrity.\")\n",
    "\n",
    "def download_images():\n",
    "    \"\"\"Download and unzip DeepWeeds images.\"\"\"\n",
    "    if not os.path.exists(IMG_DIRECTORY):\n",
    "        os.makedirs(IMG_DIRECTORY)\n",
    "        print(f\"Downloading DeepWeeds images to {IMG_ZIP_FILE}\")\n",
    "        try:\n",
    "            download_google_drive_file(IMG_GD_ID, IMG_ZIP_FILE)\n",
    "            print(\"Finished downloading images.\")\n",
    "            validate_zip_file(IMG_ZIP_FILE)\n",
    "            print(f\"Unzipping {IMG_ZIP_FILE}\")\n",
    "            with zipfile.ZipFile(IMG_ZIP_FILE, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(IMG_DIRECTORY)\n",
    "            print(\"Finished unzipping images.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error downloading or unzipping images: {e}\")\n",
    "\n",
    "def download_models():\n",
    "    \"\"\"Download and unzip DeepWeeds models.\"\"\"\n",
    "    if not os.path.exists(MODEL_DIRECTORY):\n",
    "        os.makedirs(MODEL_DIRECTORY)\n",
    "        print(f\"Downloading DeepWeeds models to {MODEL_ZIP_FILE}\")\n",
    "        try:\n",
    "            download_google_drive_file(MODEL_GD_ID, MODEL_ZIP_FILE)\n",
    "            print(\"Finished downloading models.\")\n",
    "            validate_zip_file(MODEL_ZIP_FILE)\n",
    "            print(f\"Unzipping {MODEL_ZIP_FILE}\")\n",
    "            with zipfile.ZipFile(MODEL_ZIP_FILE, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(MODEL_DIRECTORY)\n",
    "            print(\"Finished unzipping models.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error downloading or unzipping models: {e}\")\n",
    "\n",
    "def crop(img, size):\n",
    "    \"\"\"Crop the image concentrically to the desired size.\"\"\"\n",
    "    h, w, c = img.shape\n",
    "    x = int((w - size[0]) / 2)\n",
    "    y = int((h - size[1]) / 2)\n",
    "    return img[y:(y + size[1]), x:(x + size[0]), :]\n",
    "\n",
    "def crop_generator(batches, size):\n",
    "    \"\"\"Generate random crops from image batches.\"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        b, h, w, c = batch_x.shape\n",
    "        batch_crops = np.zeros((b, size[0], size[1], c))\n",
    "        for i in range(b):\n",
    "            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))\n",
    "        yield batch_crops, batch_y\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train and test ResNet50, InceptionV3, or custom model on DeepWeeds.')\n",
    "    parser.add_argument(\"command\", nargs='?', default='train', help=\"'cross_validate' or 'inference'\")\n",
    "    parser.add_argument('--model', default='resnet', help=\"'resnet', 'inception', or path to .hdf5 file.\")\n",
    "    \n",
    "    import sys\n",
    "    if any('ipykernel_launcher' in arg for arg in sys.argv) or any('jupyter' in arg for arg in sys.argv):\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    return args.command, args.model\n",
    "\n",
    "def cross_validate(model_name):\n",
    "    \"\"\"Perform k-fold cross-validation.\"\"\"\n",
    "    for k in range(FOLDS):\n",
    "        timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\n",
    "        print(f'Fold {k + 1}/{FOLDS} - {timestamp}')\n",
    "        output_directory = f\"{OUTPUT_DIRECTORY}{timestamp}/\"\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        # Load dataframes\n",
    "        train_label_file = f\"{LABEL_DIRECTORY}train_subset{k}.csv\"\n",
    "        val_label_file = f\"{LABEL_DIRECTORY}val_subset{k}.csv\"\n",
    "        test_label_file = f\"{LABEL_DIRECTORY}test_subset{k}.csv\"\n",
    "        \n",
    "        if not all(os.path.exists(f) for f in [train_label_file, val_label_file, test_label_file]):\n",
    "            raise FileNotFoundError(f\"Label files for fold {k} are missing.\")\n",
    "        \n",
    "        train_dataframe = pd.read_csv(train_label_file)\n",
    "        val_dataframe = pd.read_csv(val_label_file)\n",
    "        test_dataframe = pd.read_csv(test_label_file)\n",
    "        train_image_count = train_dataframe.shape[0]\n",
    "        val_image_count = val_dataframe.shape[0]\n",
    "        test_image_count = test_dataframe.shape[0]\n",
    "\n",
    "        # Training image augmentation\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # Validation image augmentation\n",
    "        val_data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # Test image augmentation\n",
    "        test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        # Load data generators\n",
    "        train_data_generator = train_data_generator.flow_from_dataframe(\n",
    "            train_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col='Filename',\n",
    "            y_col='Label',\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            has_ext=True,\n",
    "            classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        val_data_generator = val_data_generator.flow_from_dataframe(\n",
    "            val_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            has_ext=True,\n",
    "            classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        test_data_generator = test_data_generator.flow_from_dataframe(\n",
    "            test_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            has_ext=True,\n",
    "            shuffle=False,\n",
    "            classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        # Crop images\n",
    "        train_data_generator = crop_generator(train_data_generator, IMG_SIZE)\n",
    "        val_data_generator = crop_generator(val_data_generator, IMG_SIZE)\n",
    "\n",
    "        # Load pre-trained model\n",
    "        if model_name == \"resnet\":\n",
    "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "        elif model_name == \"inception\":\n",
    "            base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "        else:\n",
    "            raise ValueError(\"Model must be 'resnet' or 'inception'.\")\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        outputs = Dense(len(CLASSES), activation='sigmoid', name='fc9')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "        # Define callbacks\n",
    "        model_checkpoint = ModelCheckpoint(output_directory + \"lastbest-0.hdf5\", verbose=1, save_best_only=True)\n",
    "        early_stopping = EarlyStopping(patience=STOPPING_PATIENCE, restore_best_weights=True)\n",
    "        tensorboard = TensorBoard(log_dir=output_directory, histogram_freq=0, write_graph=True, write_images=False)\n",
    "        reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=LR_PATIENCE, min_lr=0.000003125)\n",
    "        csv_logger = CSVLogger(output_directory + \"training_metrics.csv\")\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=INITIAL_LR), \n",
    "                      metrics=['categorical_accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        global_epoch = 0\n",
    "        restarts = 0\n",
    "        last_best_losses = []\n",
    "        last_best_epochs = []\n",
    "        while global_epoch < MAX_EPOCH:\n",
    "            history = model.fit(\n",
    "                train_data_generator,\n",
    "                steps_per_epoch=train_image_count // BATCH_SIZE,\n",
    "                epochs=MAX_EPOCH - global_epoch,\n",
    "                validation_data=val_data_generator,\n",
    "                validation_steps=val_image_count // BATCH_SIZE,\n",
    "                callbacks=[tensorboard, model_checkpoint, early_stopping, reduce_lr, csv_logger],\n",
    "                verbose=1)\n",
    "            last_best_losses.append(min(history.history['val_loss']))\n",
    "            last_best_local_epoch = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "            last_best_epochs.append(global_epoch + last_best_local_epoch)\n",
    "            if early_stopping.stopped_epoch == 0:\n",
    "                print(f\"Completed training after {MAX_EPOCH} epochs.\")\n",
    "                break\n",
    "            else:\n",
    "                global_epoch = global_epoch + early_stopping.stopped_epoch - STOPPING_PATIENCE + 1\n",
    "                print(f\"Early stopping triggered after local epoch {early_stopping.stopped_epoch} \"\n",
    "                      f\"(global epoch {global_epoch}).\")\n",
    "                print(f\"Restarting from last best val_loss at local epoch \"\n",
    "                      f\"{early_stopping.stopped_epoch - STOPPING_PATIENCE} \"\n",
    "                      f\"(global epoch {global_epoch - STOPPING_PATIENCE}).\")\n",
    "                restarts += 1\n",
    "                model.compile(loss='binary_crossentropy', \n",
    "                              optimizer=Adam(learning_rate=INITIAL_LR / 2 ** restarts),\n",
    "                              metrics=['categorical_accuracy'])\n",
    "                model_checkpoint = ModelCheckpoint(output_directory + f\"lastbest-{restarts}.hdf5\",\n",
    "                                                   monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "        # Save last best model info\n",
    "        with open(output_directory + \"last_best_models.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Model file', 'Global epoch', 'Validation loss'])\n",
    "            for i in range(restarts + 1):\n",
    "                writer.writerow([f\"lastbest-{i}.hdf5\", last_best_epochs[i], last_best_losses[i]])\n",
    "\n",
    "        # Load the best model\n",
    "        model = load_model(output_directory + f\"lastbest-{last_best_losses.index(min(last_best_losses))}.hdf5\")\n",
    "\n",
    "        # Evaluate model\n",
    "        predictions = model.predict(test_data_generator, steps=test_image_count // BATCH_SIZE + 1)\n",
    "        y_true = test_data_generator.classes\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        y_pred[np.max(predictions, axis=1) < 1 / 9] = 8\n",
    "\n",
    "        # Save classification report and confusion matrix\n",
    "        print(classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES))\n",
    "        report = classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES, output_dict=True)\n",
    "        with open(output_directory + 'classification_report.csv', 'w') as f:\n",
    "            for key in report.keys():\n",
    "                f.write(f\"{key},{report[key]}\\n\")\n",
    "        conf_arr = confusion_matrix(y_true, y_pred, labels=CLASSES)\n",
    "        print(conf_arr)\n",
    "        np.savetxt(output_directory + \"confusion_matrix.csv\", conf_arr, delimiter=\",\")\n",
    "\n",
    "        print(f\"Finished testing fold {k + 1}\\n\")\n",
    "        K.clear_session()\n",
    "\n",
    "def inference(model):\n",
    "    \"\"\"Perform inference on DeepWeeds images.\"\"\"\n",
    "    timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\n",
    "    output_directory = f\"{OUTPUT_DIRECTORY}{timestamp}/\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Load dataframe\n",
    "    label_file = f\"{LABEL_DIRECTORY}labels.csv\"\n",
    "    if not os.path.exists(label_file):\n",
    "        raise FileNotFoundError(f\"Label file {label_file} is missing.\")\n",
    "    \n",
    "    dataframe = pd.read_csv(label_file)\n",
    "    image_count = dataframe.shape[0]\n",
    "    filenames = dataframe.Filename\n",
    "\n",
    "    preprocessing_times = []\n",
    "    inference_times = []\n",
    "    for i in range(image_count):\n",
    "        try:\n",
    "            start_time = time()\n",
    "            img = imread(os.path.join(IMG_DIRECTORY, filenames[i]))\n",
    "            img = resize(img, (224, 224))\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            img = img * 1./255\n",
    "            preprocessing_time = time() - start_time\n",
    "            start_time = time()\n",
    "            prediction = model.predict(img, batch_size=1, verbose=0)\n",
    "            y_pred = np.argmax(prediction, axis=1)\n",
    "            y_pred[np.max(prediction, axis=1) < 1/9] = 8\n",
    "            inference_time = time() - start_time\n",
    "            preprocessing_times.append(preprocessing_time)\n",
    "            inference_times.append(inference_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {filenames[i]}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save inference times\n",
    "    with open(output_directory + \"tf_inference_times.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['Filename', 'Preprocessing time (ms)', 'Inference time (ms)'])\n",
    "        for i in range(image_count):\n",
    "            writer.writerow([filenames[i], preprocessing_times[i] * 1000, inference_times[i] * 1000])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse arguments\n",
    "    command, model = parse_args()\n",
    "\n",
    "    # Download images and models\n",
    "    try:\n",
    "        download_images()\n",
    "        download_models()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in downloading resources: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Execute command\n",
    "    if command == \"cross_validate\":\n",
    "        if model not in [\"resnet\", \"inception\"]:\n",
    "            print(\"Error: Model must be 'resnet' or 'inception' for cross_validate command.\")\n",
    "            exit(1)\n",
    "        cross_validate(model)\n",
    "    elif command == \"inference\":\n",
    "        if not model.endswith(\".hdf5\"):\n",
    "            print(\"Error: You must supply a valid .hdf5 model file for inference (e.g., 'path/to/model.hdf5').\")\n",
    "            exit(1)\n",
    "        if not os.path.exists(model):\n",
    "            print(f\"Error: Model file '{model}' does not exist.\")\n",
    "            exit(1)\n",
    "        try:\n",
    "            model = load_model(model)\n",
    "            inference(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model '{model}': {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Error: Invalid command '{command}'. Use 'cross_validate' or 'inference'.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e315f1d-08b2-4de1-976b-273f35e222a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/images.zip True 2422\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(IMG_ZIP_FILE, os.path.exists(IMG_ZIP_FILE), os.path.getsize(IMG_ZIP_FILE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a5f892-3f2e-4340-bccb-5a29fcb308b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html><html><head><title>Google Drive - Virus scan warning</title><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"/><style nonce=\"mctO0zUIfXyK4FzOm0ZWgA\">.goog-link-button{p'\n"
     ]
    }
   ],
   "source": [
    "with open(IMG_ZIP_FILE, 'rb') as f:\n",
    "    print(f.read(200))  # peek at first 200 bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fca22a9-f46c-4c1e-9758-9b2796ba3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(IMG_ZIP_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352dc1a-ecf8-4071-a1a1-92e0373be21b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
