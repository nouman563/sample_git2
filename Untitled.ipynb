{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c7bee-7c0d-4ce0-bae0-a9e696aa4703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid command 'C:\\Users\\mohdn\\AppData\\Roaming\\jupyter\\runtime\\kernel-d5b9098a-ef63-4940-8bd0-19e2af958035.json' detected. Likely due to Jupyter passing extra arguments: ['C:\\\\Users\\\\mohdn\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\mohdn\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-d5b9098a-ef63-4940-8bd0-19e2af958035.json']\n",
      "Please specify command as 'cross_validate' or 'inference'.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Global paths\n",
    "OUTPUT_DIRECTORY = \"./outputs/\"\n",
    "LABEL_DIRECTORY = \"./labels/\"\n",
    "MODEL_DIRECTORY = \"./models/\"\n",
    "MODEL_GD_ID = \"1MRbN5hXOTYnw7-71K-2vjY01uJ9GkQM5\"\n",
    "MODEL_ZIP_FILE = \"./models/models.zip\"\n",
    "IMG_DIRECTORY = \"./images/\"\n",
    "IMG_GD_ID = \"1xnK3B6K6KekDI55vwJ0vnc2IGoDga9cj\"\n",
    "IMG_ZIP_FILE = \"./images/images.zip\"\n",
    "\n",
    "# Global variables\n",
    "RAW_IMG_SIZE = (256, 256)\n",
    "IMG_SIZE = (224, 224)\n",
    "INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "MAX_EPOCH = 200\n",
    "BATCH_SIZE = 32\n",
    "FOLDS = 5\n",
    "STOPPING_PATIENCE = 32\n",
    "LR_PATIENCE = 16\n",
    "INITIAL_LR = 0.0001\n",
    "CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "CLASS_NAMES = ['Chinee Apple', 'Lantana', 'Parkinsonia', 'Parthenium', \n",
    "               'Prickly Acacia', 'Rubber Vine', 'Siam Weed', 'Snake Weed', 'Negatives']\n",
    "\n",
    "def download_google_drive_file(file_id, destination):\n",
    "    \"\"\"Download a file from Google Drive.\"\"\"\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': file_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    \"\"\"Extract confirmation token from response cookies.\"\"\"\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    \"\"\"Save response content to a file.\"\"\"\n",
    "    CHUNK_SIZE = 32768\n",
    "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "    try:\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to save file to {destination}: {e}\")\n",
    "\n",
    "def validate_zip_file(file_path):\n",
    "    \"\"\"Check if a file is a valid ZIP file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} does not exist.\")\n",
    "    if not zipfile.is_zipfile(file_path):\n",
    "        raise ValueError(f\"{file_path} is not a valid ZIP file. Check the download URL or file integrity.\")\n",
    "\n",
    "def download_images():\n",
    "    \"\"\"Download and unzip DeepWeeds images.\"\"\"\n",
    "    if not os.path.exists(IMG_DIRECTORY):\n",
    "        os.makedirs(IMG_DIRECTORY)\n",
    "        print(f\"Downloading DeepWeeds images to {IMG_ZIP_FILE}\")\n",
    "        try:\n",
    "            download_google_drive_file(IMG_GD_ID, IMG_ZIP_FILE)\n",
    "            print(\"Finished downloading images.\")\n",
    "            validate_zip_file(IMG_ZIP_FILE)\n",
    "            print(f\"Unzipping {IMG_ZIP_FILE}\")\n",
    "            with zipfile.ZipFile(IMG_ZIP_FILE, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(IMG_DIRECTORY)\n",
    "            print(\"Finished unzipping images.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error downloading or unzipping images: {e}\")\n",
    "\n",
    "def download_models():\n",
    "    \"\"\"Download and unzip DeepWeeds models.\"\"\"\n",
    "    if not os.path.exists(MODEL_DIRECTORY):\n",
    "        os.makedirs(MODEL_DIRECTORY)\n",
    "        print(f\"Downloading DeepWeeds models to {MODEL_ZIP_FILE}\")\n",
    "        try:\n",
    "            download_google_drive_file(MODEL_GD_ID, MODEL_ZIP_FILE)\n",
    "            print(\"Finished downloading models.\")\n",
    "            validate_zip_file(MODEL_ZIP_FILE)\n",
    "            print(f\"Unzipping {MODEL_ZIP_FILE}\")\n",
    "            with zipfile.ZipFile(MODEL_ZIP_FILE, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(MODEL_DIRECTORY)\n",
    "            print(\"Finished unzipping models.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error downloading or unzipping models: {e}\")\n",
    "\n",
    "def crop(img, size):\n",
    "    \"\"\"Crop the image concentrically to the desired size.\"\"\"\n",
    "    h, w, c = img.shape\n",
    "    x = int((w - size[0]) / 2)\n",
    "    y = int((h - size[1]) / 2)\n",
    "    return img[y:(y + size[1]), x:(x + size[0]), :]\n",
    "\n",
    "def crop_generator(batches, size):\n",
    "    \"\"\"Generate random crops from image batches.\"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        b, h, w, c = batch_x.shape\n",
    "        batch_crops = np.zeros((b, size[0], size[1], c))\n",
    "        for i in range(b):\n",
    "            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))\n",
    "        yield batch_crops, batch_y\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments, with special handling for Jupyter environments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train and test ResNet50, InceptionV3, or custom model on DeepWeeds.')\n",
    "    parser.add_argument(\"command\", nargs='?', default='cross_validate', \n",
    "                        help=\"'cross_validate' or 'inference'\")\n",
    "    parser.add_argument('--model', default='resnet', \n",
    "                        help=\"'resnet', 'inception', or path to .hdf5 file for inference\")\n",
    "    \n",
    "    # Check if running in Jupyter\n",
    "    is_jupyter = any('ipykernel_launcher' in arg for arg in sys.argv) or any('jupyter' in arg for arg in sys.argv)\n",
    "    \n",
    "    if is_jupyter:\n",
    "        # In Jupyter, use default arguments or prompt for input if needed\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        # Filter out Jupyter-specific arguments\n",
    "        if args.command not in ['cross_validate', 'inference']:\n",
    "            print(f\"Invalid command '{args.command}' detected. Likely due to Jupyter passing extra arguments: {sys.argv}\")\n",
    "            print(\"Please specify command as 'cross_validate' or 'inference'.\")\n",
    "            command = input(\"Enter command (cross_validate or inference): \").strip()\n",
    "            while command not in ['cross_validate', 'inference']:\n",
    "                print(\"Error: Command must be 'cross_validate' or 'inference'.\")\n",
    "                command = input(\"Enter command (cross_validate or inference): \").strip()\n",
    "            args.command = command\n",
    "            \n",
    "            # For inference, prompt for model file if necessary\n",
    "            if args.command == 'inference':\n",
    "                if not args.model.endswith('.hdf5'):\n",
    "                    print(f\"Invalid model '{args.model}'. For inference, provide a valid .hdf5 file path.\")\n",
    "                    args.model = input(\"Enter path to .hdf5 model file: \").strip()\n",
    "                    while not args.model.endswith('.hdf5'):\n",
    "                        print(\"Error: Model must be a .hdf5 file.\")\n",
    "                        args.model = input(\"Enter path to .hdf5 model file: \").strip()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    return args.command, args.model\n",
    "\n",
    "def cross_validate(model_name):\n",
    "    \"\"\"Perform k-fold cross-validation.\"\"\"\n",
    "    for k in range(FOLDS):\n",
    "        timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\n",
    "        print(f'Fold {k + 1}/{FOLDS} - {timestamp}')\n",
    "        output_directory = f\"{OUTPUT_DIRECTORY}{timestamp}/\"\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        # Load dataframes\n",
    "        train_label_file = f\"{LABEL_DIRECTORY}train_subset{k}.csv\"\n",
    "        val_label_file = f\"{LABEL_DIRECTORY}val_subset{k}.csv\"\n",
    "        test_label_file = f\"{LABEL_DIRECTORY}test_subset{k}.csv\"\n",
    "        \n",
    "        if not all(os.path.exists(f) for f in [train_label_file, val_label_file, test_label_file]):\n",
    "            raise FileNotFoundError(f\"Label files for fold {k} are missing.\")\n",
    "        \n",
    "        train_dataframe = pd.read_csv(train_label_file)\n",
    "        val_dataframe = pd.read_csv(val_label_file)\n",
    "        test_dataframe = pd.read_csv(test_label_file)\n",
    "        train_image_count = train_dataframe.shape[0]\n",
    "        val_image_count = val_dataframe.shape[0]\n",
    "        test_image_count = test_dataframe.shape[0]\n",
    "\n",
    "        # Training image augmentation\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # Validation image augmentation\n",
    "        val_data_generator = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # Test image augmentation\n",
    "        test_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        # Load data generators\n",
    "        train_data_generator = train_data_generator.flow_from_dataframe(\n",
    "            train_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col='Filename',\n",
    "            y_col='Label',\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            has_ext=True,\n",
    "            classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        val_data_generator = val_data_generator.flow_from_dataframe(\n",
    "            val_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            has_ext=True,\n",
    "            classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        test_data_generator = test_data_generator.flow_from_dataframe(\n",
    "            test_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            has_ext=True,\n",
    "            shuffle=False,\n",
    "            classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        # Crop images\n",
    "        train_data_generator = crop_generator(train_data_generator, IMG_SIZE)\n",
    "        val_data_generator = crop_generator(val_data_generator, IMG_SIZE)\n",
    "\n",
    "        # Load pre-trained model\n",
    "        if model_name == \"resnet\":\n",
    "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "        elif model_name == \"inception\":\n",
    "            base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "        else:\n",
    "            raise ValueError(\"Model must be 'resnet' or 'inception'.\")\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        outputs = Dense(len(CLASSES), activation='sigmoid', name='fc9')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "        # Define callbacks\n",
    "        model_checkpoint = ModelCheckpoint(output_directory + \"lastbest-0.hdf5\", verbose=1, save_best_only=True)\n",
    "        early_stopping = EarlyStopping(patience=STOPPING_PATIENCE, restore_best_weights=True)\n",
    "        tensorboard = TensorBoard(log_dir=output_directory, histogram_freq=0, write_graph=True, write_images=False)\n",
    "        reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=LR_PATIENCE, min_lr=0.000003125)\n",
    "        csv_logger = CSVLogger(output_directory + \"training_metrics.csv\")\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=INITIAL_LR), \n",
    "                      metrics=['categorical_accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        global_epoch = 0\n",
    "        restarts = 0\n",
    "        last_best_losses = []\n",
    "        last_best_epochs = []\n",
    "        while global_epoch < MAX_EPOCH:\n",
    "            history = model.fit(\n",
    "                train_data_generator,\n",
    "                steps_per_epoch=train_image_count // BATCH_SIZE,\n",
    "                epochs=MAX_EPOCH - global_epoch,\n",
    "                validation_data=val_data_generator,\n",
    "                validation_steps=val_image_count // BATCH_SIZE,\n",
    "                callbacks=[tensorboard, model_checkpoint, early_stopping, reduce_lr, csv_logger],\n",
    "                verbose=1)\n",
    "            last_best_losses.append(min(history.history['val_loss']))\n",
    "            last_best_local_epoch = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "            last_best_epochs.append(global_epoch + last_best_local_epoch)\n",
    "            if early_stopping.stopped_epoch == 0:\n",
    "                print(f\"Completed training after {MAX_EPOCH} epochs.\")\n",
    "                break\n",
    "            else:\n",
    "                global_epoch = global_epoch + early_stopping.stopped_epoch - STOPPING_PATIENCE + 1\n",
    "                print(f\"Early stopping triggered after local epoch {early_stopping.stopped_epoch} \"\n",
    "                      f\"(global epoch {global_epoch}).\")\n",
    "                print(f\"Restarting from last best val_loss at local epoch \"\n",
    "                      f\"{early_stopping.stopped_epoch - STOPPING_PATIENCE} \"\n",
    "                      f\"(global epoch {global_epoch - STOPPING_PATIENCE}).\")\n",
    "                restarts += 1\n",
    "                model.compile(loss='binary_crossentropy', \n",
    "                              optimizer=Adam(learning_rate=INITIAL_LR / 2 ** restarts),\n",
    "                              metrics=['categorical_accuracy'])\n",
    "                model_checkpoint = ModelCheckpoint(output_directory + f\"lastbest-{restarts}.hdf5\",\n",
    "                                                   monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "        # Save last best model info\n",
    "        with open(output_directory + \"last_best_models.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Model file', 'Global epoch', 'Validation loss'])\n",
    "            for i in range(restarts + 1):\n",
    "                writer.writerow([f\"lastbest-{i}.hdf5\", last_best_epochs[i], last_best_losses[i]])\n",
    "\n",
    "        # Load the best model\n",
    "        model = load_model(output_directory + f\"lastbest-{last_best_losses.index(min(last_best_losses))}.hdf5\")\n",
    "\n",
    "        # Evaluate model\n",
    "        predictions = model.predict(test_data_generator, steps=test_image_count // BATCH_SIZE + 1)\n",
    "        y_true = test_data_generator.classes\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        y_pred[np.max(predictions, axis=1) < 1 / 9] = 8\n",
    "\n",
    "        # Save classification report and confusion matrix\n",
    "        print(classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES))\n",
    "        report = classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES, output_dict=True)\n",
    "        with open(output_directory + 'classification_report.csv', 'w') as f:\n",
    "            for key in report.keys():\n",
    "                f.write(f\"{key},{report[key]}\\n\")\n",
    "        conf_arr = confusion_matrix(y_true, y_pred, labels=CLASSES)\n",
    "        print(conf_arr)\n",
    "        np.savetxt(output_directory + \"confusion_matrix.csv\", conf_arr, delimiter=\",\")\n",
    "\n",
    "        print(f\"Finished testing fold {k + 1}\\n\")\n",
    "        K.clear_session()\n",
    "\n",
    "def inference(model):\n",
    "    \"\"\"Perform inference on DeepWeeds images.\"\"\"\n",
    "    timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\n",
    "    output_directory = f\"{OUTPUT_DIRECTORY}{timestamp}/\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Load dataframe\n",
    "    label_file = f\"{LABEL_DIRECTORY}labels.csv\"\n",
    "    if not os.path.exists(label_file):\n",
    "        raise FileNotFoundError(f\"Label file {label_file} is missing.\")\n",
    "    \n",
    "    dataframe = pd.read_csv(label_file)\n",
    "    image_count = dataframe.shape[0]\n",
    "    filenames = dataframe.Filename\n",
    "\n",
    "    preprocessing_times = []\n",
    "    inference_times = []\n",
    "    for i in range(image_count):\n",
    "        try:\n",
    "            start_time = time()\n",
    "            img = imread(os.path.join(IMG_DIRECTORY, filenames[i]))\n",
    "            img = resize(img, (224, 224))\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            img = img * 1./255\n",
    "            preprocessing_time = time() - start_time\n",
    "            start_time = time()\n",
    "            prediction = model.predict(img, batch_size=1, verbose=0)\n",
    "            y_pred = np.argmax(prediction, axis=1)\n",
    "            y_pred[np.max(prediction, axis=1) < 1/9] = 8\n",
    "            inference_time = time() - start_time\n",
    "            preprocessing_times.append(preprocessing_time)\n",
    "            inference_times.append(inference_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {filenames[i]}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save inference times\n",
    "    with open(output_directory + \"tf_inference_times.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['Filename', 'Preprocessing time (ms)', 'Inference time (ms)'])\n",
    "        for i in range(image_count):\n",
    "            writer.writerow([filenames[i], preprocessing_times[i] * 1000, inference_times[i] * 1000])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse arguments\n",
    "    command, model = parse_args()\n",
    "\n",
    "    # Download images and models\n",
    "    try:\n",
    "        download_images()\n",
    "        download_models()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in downloading resources: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Execute command\n",
    "    if command == \"cross_validate\":\n",
    "        if model not in [\"resnet\", \"inception\"]:\n",
    "            print(\"Error: Model must be 'resnet' or 'inception' for cross_validate command.\")\n",
    "            exit(1)\n",
    "        cross_validate(model)\n",
    "    elif command == \"inference\":\n",
    "        if not model.endswith(\".hdf5\"):\n",
    "            print(\"Error: You must supply a valid .hdf5 model file for inference (e.g., 'path/to/model.hdf5').\")\n",
    "            exit(1)\n",
    "        if not os.path.exists(model):\n",
    "            print(f\"Error: Model file '{model}' does not exist.\")\n",
    "            exit(1)\n",
    "        try:\n",
    "            model = load_model(model)\n",
    "            inference(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model '{model}': {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Error: Invalid command '{command}'. Use 'cross_validate' or 'inference'.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e94acb-015e-4380-b6b4-9fa4be7c43b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
